---
title: Sims and Uhlig (1991) Replication
author: Francis J. DiTraglia
date: '2024-07-15'
slug: sims-and-uhlig-1991-replication
categories: [econometrics, statistics]
tags: [time series, unit root, Bayesian, R]
subtitle: ''
summary: ''
authors: []
lastmod: '2024-07-15T11:04:32+01:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---



<p>As a teaser for our upcoming (2024-07-23) virtual reading group session on Bayesian macro / time series econometrics, this post replicates a classic paper by <a href="https://doi.org/10.2307/2938280">Sims &amp; Uhlig (1991)</a> contrasting Bayesian and Frequentist inferences for a unit root. In the post I’ll focus on explaining and implementing the authors’ simulation design. In the reading group session (and possibly a future post) we’ll talk more about the paper’s implications for the Bayesian-Frequentist debate and relate it to more recent work by <a href="https://anorets.github.io/papers/covpriors.pdf">Mueller &amp; Norets (2016)</a>. We’ll also be joined by special guest <a href="https://web.sas.upenn.edu/schorf/">Frank Schorfheide</a> who will help guide us through the recent literature on Bayesian approaches to VARs, including <a href="https://doi.org/10.1162/REST_a_00483">Giannone et al (2015)</a> and <a href="https://doi.org/10.1080/01621459.2018.1483826">(2019)</a>. If you’re an Oxford student or staff member, you can sign up for the reading group <a href="https://edstem.org/us/join/6j2hay">here</a>. Otherwise, send me an email and I’ll add you manually.</p>
<div id="a-simple-example" class="section level2">
<h2>A Simple Example</h2>
<p>To set the stage for Sims &amp; Uhlig (1991), consider the following simple example: <span class="math inline">\(X_1, X_2, \dots X_{100} \sim \text{Normal}(\mu, \sigma^2)\)</span> where <span class="math inline">\(\mu\)</span> is unknown but <span class="math inline">\(\sigma\)</span> is known to equal <span class="math inline">\(1\)</span>. Let <span class="math inline">\(\bar{X} = \frac{1}{100} \sum_{i=1}^{100} X_i\)</span> be the sample mean. Then <span class="math inline">\(\bar{X} \pm 0.2\)</span> is an approximate 95% Frequentist confidence interval for <span class="math inline">\(\mu\)</span>. In words: among 95% of the possible datasets that we could potentially observe, the interval <span class="math inline">\(\bar{X} \pm 0.2\)</span> will cover the true, unknown value of <span class="math inline">\(\mu\)</span>; in the remaining <span class="math inline">\(5\%\)</span> of datasets, the interval will not cover <span class="math inline">\(\mu\)</span>.</p>
<p>The Frequentist interval conditions on <span class="math inline">\(\mu\)</span> and treats <span class="math inline">\(\bar{X}\)</span> as random. In contrast, a Bayesian credible interval conditions on <span class="math inline">\(\bar{X}\)</span> and treats <span class="math inline">\(\mu\)</span> as random. This doesn’t require us to believe that <span class="math inline">\(\mu\)</span> is “really” random. Bayesian reasoning simply uses the language of probability to express uncertainty about <em>any quantity that we cannot observe</em>. Let <span class="math inline">\(\bar{x}\)</span> be the observed value of <span class="math inline">\(\bar{X}\)</span>. Under a vague prior for <span class="math inline">\(\mu\)</span>, e.g. a Normal(0, 100) distribution, the 95% Bayesian <a href="https://en.wikipedia.org/wiki/Credible_interval">highest posterior density interval</a> for <span class="math inline">\(\mu\)</span> is approximately <span class="math inline">\(\bar{x} \pm 0.2\)</span>. In words: given that we have observed <span class="math inline">\(\bar{X} = \bar{x}\)</span>, there is a 95% probability that <span class="math inline">\(\mu\)</span> lies in the interval <span class="math inline">\(\bar{x} \pm 0.2\)</span>.</p>
<p>The comforting thing about this example is that, regardless of whether we choose a Bayesian or Frequentist perspective, our inference remains the same: compute the sample mean, then add and subtract <span class="math inline">\(0.2\)</span>. This means that the Frequentist interval inherits all the nice properties of Bayesian inferences, and the Bayesian interval has correct Frequentist coverage. This equivalence between Bayesian and Frequentist methods crops up in many simple examples, especially in situations where the sample size is large. But in more complex settings, the two approaches can give radically different answers. And to head off a common mis-understanding, this <em>isn’t</em> because Bayesians use priors. In the limit as we accumulate more and more data, the influence of the prior wanes. The key difference is that Bayesian inference adheres to the <a href="https://en.wikipedia.org/wiki/Likelihood_principle">likelihood principle</a>, whereas common Frequentist methods do not.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></p>
</div>
<div id="a-not-so-simple-example" class="section level2">
<h2>A Not-so-simple Example</h2>
<p>Sims &amp; Uhlig consider the AR(1) model
<span class="math display">\[
y_t = \rho y_{t-1} + \varepsilon_t, \quad \varepsilon_t \sim \text{iid Normal}(0, 1)
\]</span>
and the conditional maximum likelihood estimator given the initial <span class="math inline">\(y_0\)</span>, namely
<span class="math display">\[
\widehat{\rho} = \frac{\sum_{t=1}^T y_{t-1} y_t}{\sum_{t=1}^T y_{t-1}^2}.
\]</span>
Their simulation contrasts the Frequentist sampling distribution of <span class="math inline">\(\widehat{\rho}|\rho\)</span> with the Bayesian posterior distribution of <span class="math inline">\(\rho|\widehat{\rho}\)</span> under a flat prior on <span class="math inline">\(\rho\)</span>. When <span class="math inline">\(\rho\)</span> is near one, these two distributions differ markedly: while the Bayesian posterior is always symmetric and centered at <span class="math inline">\(\widehat{\rho} = \widehat{\rho}\)</span>, the Frequentist sampling distribution is highly skewed when <span class="math inline">\(\rho\)</span> is close to one. This shows that the Bayesian-Frequentist equivalence we found in our simple population mean example from above breaks down completely in this more complex example.</p>
<p>Sims &amp; Uhlig argue that the Bayesian posterior provides a much more sensible and useful characterization of the information contained in the data and after reading the paper, I’m inclined to agree. My replication code follows below, along with plots of the joint distribution of <span class="math inline">\((\rho, \widehat{\rho})\)</span> under a uniform prior for <span class="math inline">\(\rho\)</span> and the conditional distributions <span class="math inline">\(\widehat{\rho}|\rho=1\)</span> (Frequentist Sampling Distribution) and <span class="math inline">\(\rho|\widehat{\rho} = 1\)</span> (Bayesian Posterior).<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></p>
</div>
<div id="the-replication" class="section level2">
<h2>The Replication</h2>
<pre class="r"><code>#-------------------------------------------------------------------------------
# Sims, C. A., &amp; Uhlig, H. (1991). Understanding unit rooters: A helicopter tour
#
# (See also: Example 6.10.6 from Poirier &quot;Intermediate Statistics and &#39;Metrics&quot;)
#-------------------------------------------------------------------------------
# In the next section we will proceed to construct, by Monte Carlo, an estimated
# joint pdf for \rho and \hat{\rho} under a uniform prior pdf on \rho. We choose
# 31 values of \rho, from 0.8 to 1.1 at intervals of 0.01. We draw 10000 100 x 1
# iid N(0,1) vectors of random variables to use a realizations of \epsilon. For
# each of the 10000 \epsilon vectors and each of the 31 \rho values, we 
# construct a y vector with y(0) = 0, y(t) generated by equation (1). 
#
# Equation (1): y(t) = \rho y(t-1) + \epsilon(t), t = 0, ..., T
#
# For each of these y vectors, we construct \hat{\rho}. Using as bins the
# intervals [-\infty, 0.795), [0.795, 0.805), [0.805, 0.815), etc. we construct
# a histogram that estimates the pdf of \hat{rho} for each fixed \rho value. 
# When these histograms are lined up side by side, they form a surface that is
# the joint pdf for \rho and \hat{\rho} under a flat prior on \rho.
#-------------------------------------------------------------------------------
set.seed(1693)

library(tidyverse)
library(tictoc)
library(patchwork)

draw_rho_hat &lt;- function(rho) {
# Carry out the simulation once for a fixed value of rho; return rho_hat
  nT &lt;- 100
  y &lt;- rep(0, nT + 1) 
  for (t in 2:(nT + 1)) {
    y[t] &lt;- rho * y[t - 1] + rnorm(1)
  }
  y_t &lt;- y[-1]
  y_tminus1 &lt;- y[-length(y)]
  sum(y_t * y_tminus1) / sum(y_tminus1^2)
}

# Function to run the simulation for a fixed value of rho (10000 times)
run_sim &lt;- \(rho) map_dbl(1:1e4, \(i) draw_rho_hat(rho))

tic()
foo &lt;- run_sim(0.9)
toc() # ~0.6 seconds on my machine</code></pre>
<pre><code>## 0.595 sec elapsed</code></pre>
<pre class="r"><code># Full sequence of rho values from Sims &amp; Uhlig (1991)
rho &lt;- seq(from = 0.8, to = 1.1, by = 0.01)

tic()
results &lt;- tibble(rho = rho, 
                  rho_hat = map(rho, run_sim)) # List columns
toc() # ~17 seconds on my machine (1991 was a long time ago!)</code></pre>
<pre><code>## 16.814 sec elapsed</code></pre>
<pre class="r"><code># The results tibble uses a list column for rho_hat. This is convenient for 
# making histograms of the frequentist sampling distribution (rho fixed) but
# not for making histograms of the Bayesian posterior (rho_hat) fixed. For the
# latter, we will use the unnest() function to &quot;expand&quot; the list column rho_hat
# into a regular column. This is the &quot;joint&quot; distribution of rho and rho_hat.
joint &lt;- results |&gt; 
  unnest(rho_hat)

joint |&gt; 
  ggplot(aes(x = rho, y = rho_hat)) +
  geom_density2d_filled() + 
  coord_cartesian(ylim = c(0.8, 1.1)) + # Restrict rho_hat axis
  labs(title = &quot;Joint Distribution&quot;,
       x = expression(rho),
       y = expression(hat(rho))) +
  theme_minimal()</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<pre class="r"><code>joint |&gt; 
  filter(rho_hat &gt;= 0.995 &amp; rho_hat &lt; 1.005) |&gt;
  ggplot(aes(x = rho)) +
  geom_histogram(binwidth = 0.01, fill = &quot;skyblue&quot;, color = &quot;black&quot;) +
  labs(title = expression(hat(rho) == 1),
       x = expression(rho),
       y = &quot;Frequency&quot;) +
  theme_minimal()</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-1-2.png" width="672" /></p>
<pre class="r"><code>joint |&gt; 
  filter(rho == 1) |&gt; 
  ggplot(aes(x = rho_hat)) +
  geom_histogram(binwidth = 0.01, fill = &quot;skyblue&quot;, color = &quot;black&quot;) +
  labs(title = expression(rho == 1),
       x = expression(hat(rho)),
       y = &quot;Frequency&quot;) +
  theme_minimal()</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-1-3.png" width="672" /></p>
<pre class="r"><code># Function that makes the preceding two plots, puts them side-by-side and lets
# the user specify the value of rho/rho_hat that we condition on:
plot_Bayes_vs_Freq &lt;- \(r) {
  p1 &lt;- joint |&gt; 
    filter(rho_hat &gt;= r - 0.005 &amp; rho_hat &lt; r + 0.005) |&gt; 
    ggplot(aes(x = rho)) +
    geom_histogram(aes(y = after_stat(density)), 
                   binwidth = 0.01, fill = &quot;skyblue&quot;, color = &quot;black&quot;) +
    geom_vline(xintercept = r, color = &quot;red&quot;, linetype = &quot;dashed&quot;, linewidth = 1) +
    labs(title = bquote(hat(rho) == .(round(r, 3))),
         x = expression(rho)) +
    theme_minimal()
  
  p2 &lt;- joint |&gt; 
    filter(rho &gt;= r - 0.005 &amp; rho &lt; r + 0.005) |&gt; 
    ggplot(aes(x = rho_hat)) +
    geom_histogram(aes(y = after_stat(density)), 
                   binwidth = 0.01, fill = &quot;skyblue&quot;, color = &quot;black&quot;) +
    geom_vline(xintercept = r, color = &quot;red&quot;, linetype = &quot;dashed&quot;, linewidth = 1) +
    labs(title = bquote(rho == .(round(r, 3))),
         x = expression(hat(rho))) +
    theme_minimal()
  
  p1 + p2
}

plot_Bayes_vs_Freq(0.98)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-1-4.png" width="672" /></p>
<pre class="r"><code>plot_Bayes_vs_Freq(0.99)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-1-5.png" width="672" /></p>
<pre class="r"><code>plot_Bayes_vs_Freq(1.0)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-1-6.png" width="672" /></p>
<pre class="r"><code>plot_Bayes_vs_Freq(1.01)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-1-7.png" width="672" /></p>
<pre class="r"><code>plot_Bayes_vs_Freq(1.02)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-1-8.png" width="672" /></p>
</div>
<div class="footnotes footnotes-end-of-document">
<hr />
<ol>
<li id="fn1"><p>A detailed discussion of the likelihood principle would require at least a whole post of its own. If you want to learn more, I highly recommend the classic monograph by <a href="https://external.dandelon.com/download/attachments/dandelon/ids/DE004496C87987070706BC125794B00403A1A.pdf">Berger &amp; Wolpert</a>.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>For further discussion of Sims and Uhlig’s illuminating simulation experiment, see Chapter 6 of <a href="https://mitpress.mit.edu/9780262660945/intermediate-statistics-and-econometrics/">Poirier</a>.<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
