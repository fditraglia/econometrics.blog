---
title: Beyond Classical Measurement Error
author: Francis J. DiTraglia
date: '2021-08-23'
slug: beyond-classical-measurement-error
categories: [measurement error]
tags: []
subtitle: ''
summary: ''
authors: []
lastmod: '2021-08-23T13:01:22+01:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<p>Pop Quiz: If <span class="math inline">\(D^*\)</span> and <span class="math inline">\(D\)</span> are binary random variables and <span class="math inline">\(D\)</span> is a noisy measure of <span class="math inline">\(D^*\)</span>, is it possible for the measurement error <span class="math inline">\(W \equiv D - D^*\)</span> to be <em>classical</em>? Explain why or why not. (Answer below)</p>
<div id="classical-measurement-error" class="section level1">
<h1>Classical Measurement Error</h1>
<p>Classical measurement error is a problem that is easy to understand and relatively easy to address.
Roughly speaking, classical measurement error refers to a situation in which the variable we observe equals the truth plus noise
<span class="math display">\[
\text{Observed} = \text{Truth} + \text{Noise}
\]</span>
where the noise is unrelated to the truth and “everything else.”
(I’ll be precise about the meaning of “unrelated” and “everything else” in a moment.)
Mis-measuring a regressor <span class="math inline">\(X\)</span> in this way biases the OLS slope estimator towards zero (<em>attenuation bias</em>) but we can correct for this with a valid instrument. Mis-measuring the outcome <span class="math inline">\(Y\)</span> increases standard errors but doesn’t bias the OLS estimator. You can find all the details in your favorite introductory econometrics textbook, but in the interest of making this post self-contained, here’s a quick review.</p>
<div id="least-squares-attenuation-bias" class="section level2">
<h2>Least Squares Attenuation Bias</h2>
<p>Suppose that we want to learn the slope coefficient from a population linear regression of <span class="math inline">\(Y\)</span> on <span class="math inline">\(X^*\)</span>:
<span class="math display">\[
\beta \equiv \frac{\text{Cov}(Y,X^*)}{\text{Var}(X^*)}. 
\]</span>
Unfortunately we observe not <span class="math inline">\(X^*\)</span> but a noisy measure <span class="math inline">\(X = X^* + W_X\)</span> where <span class="math inline">\(W_X\)</span> is uncorrelated with both <span class="math inline">\(X^*\)</span> and <span class="math inline">\(Y\)</span>.
Then
<span class="math display">\[
\begin{aligned}
\text{Cov}(Y, X) &amp;= \text{Cov}(Y, X^* + W_X) = \text{Cov}(Y, X^*)\\
\text{Var}(X) &amp;= \text{Var}(X^* + W_X) = \text{Var}(X^*) + \text{Var}(W_X).
\end{aligned}
\]</span>
Now, define the <em>reliability ratio</em> <span class="math inline">\(\lambda\)</span> as follows:
<span class="math display">\[
\lambda \equiv \frac{\text{Var}(X^*)}{\text{Var}(X^*) + \text{Var}(W_X)}.
\]</span>
Measurement error means that <span class="math inline">\(\text{Var}(W_X)\)</span> is positive.
Since variances can’t be negative, this implies <span class="math inline">\(0 &lt; \lambda &lt; 1\)</span>.
Combining our definition of <span class="math inline">\(\lambda\)</span> with the expressions for <span class="math inline">\(\text{Cov}(Y,X)\)</span> and <span class="math inline">\(\text{Var}(X)\)</span> from above,
<span class="math display">\[
\begin{aligned}
\frac{\text{Cov}(Y,X)}{\text{Var}(X)} &amp;= \frac{\text{Cov}(Y, X^*)}{\text{Var}(X^*) + \text{Var}(W_X)} \\
&amp;=\frac{\text{Var}(X^*)}{\text{Var}(X^*) + \text{Var}(W_X)}\cdot \frac{\text{Cov}(Y, X^*)}{\text{Var}(X^*)}\\
&amp;= \lambda \beta
\end{aligned}
\]</span>
so we see that regressing <span class="math inline">\(Y\)</span> on <span class="math inline">\(X\)</span> gives <span class="math inline">\(\lambda \beta\)</span> rather than <span class="math inline">\(\beta\)</span>.
Since <span class="math inline">\(0 &lt; \lambda &lt; 1\)</span>, this phenomenon is called <em>least squares attenuation bias</em>: <span class="math inline">\(\lambda \beta\)</span> has the same sign as <span class="math inline">\(\beta\)</span> but is smaller in magnitude.
The greater the extent of measurement error, the larger the variance of <span class="math inline">\(W_X\)</span> and the smaller that <span class="math inline">\(|\lambda \beta|\)</span> becomes.</p>
</div>
<div id="instrumental-variables-to-the-rescue" class="section level2">
<h2>Instrumental variables to the rescue</h2>
<p>Suppose that <span class="math inline">\(Y = \alpha + \beta X^* + U\)</span> where <span class="math inline">\(X = X^* + W_X\)</span> as above.
Now suppose that we can find a variable <span class="math inline">\(Z\)</span> that is correlated with <span class="math inline">\(X^*\)</span> but uncorrelated with <span class="math inline">\(U\)</span> and <span class="math inline">\(W_X\)</span>.
Then
<span class="math display">\[
\begin{aligned}
\text{Cov}(Y,Z) &amp;= \text{Cov}(\alpha + \beta X^* + U, Z) = \beta\text{Cov}(X^*,Z)\\
\text{Cov}(X,Z) &amp;= \text{Cov}(X^* + W_X, Z) = \text{Cov}(X^*,Z)
\end{aligned}
\]</span>
so that <span class="math inline">\(\beta = \text{Cov}(Y, Z) / \text{Cov}(X,Z)\)</span>.
If <span class="math inline">\(X^*\)</span> is measured with classical measurement error, a simple instrumental variables regression solves the problem of attenuation bias.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>
Notice that we haven’t said anything about <span class="math inline">\(U\)</span> in relation to <span class="math inline">\(X^*\)</span>.
If <span class="math inline">\(\beta\)</span> is the population linear regression slope, then <span class="math inline">\(U\)</span> is uncorrelated with <span class="math inline">\(X^*\)</span> by definition.
But this derivation still goes through if <span class="math inline">\(Y = \alpha + \beta X^* + U\)</span> is a <em>causal model</em> in which <span class="math inline">\(X^*\)</span> is correlated with <span class="math inline">\(U\)</span>, e.g. if <span class="math inline">\(Y\)</span> is wage and <span class="math inline">\(X^*\)</span> is years of schooling, in which case <span class="math inline">\(U\)</span> might be “unobserved ability.”
In this way, a <em>single</em> valid instrument can serve “double-duty,” eliminating both attenuation bias and selection bias.</p>
</div>
<div id="measurement-error-in-the-outcome" class="section level2">
<h2>Measurement error in the outcome</h2>
<p>Now suppose that <span class="math inline">\(X^*\)</span> is observed but the true <em>outcome</em> <span class="math inline">\(Y^*\)</span> is not: we only observe a noisy measure <span class="math inline">\(Y = Y^* + W_Y\)</span>.
If <span class="math inline">\(W_Y\)</span> is uncorrelated with <span class="math inline">\(X^*\)</span>,
<span class="math display">\[
\frac{\text{Cov}(Y,X^*)}{\text{Var}(X^*)} = \frac{\text{Cov}(Y^* + W_Y, X^*)}{\text{Var}(X^*)} = \frac{\text{Cov}(Y^*,X^*)}{\text{Var}(X^*)} 
\]</span></p>
<p>so we’ll obtain the same slope from a regression of <span class="math inline">\(Y\)</span> on <span class="math inline">\(X^*\)</span> as we would from a regression of <span class="math inline">\(Y^*\)</span> on <span class="math inline">\(X^*\)</span>.
Classical measurement error in the outcome variable doesn’t introduce a bias.</p>
</div>
</div>
<div id="solution-to-the-pop-quiz" class="section level1">
<h1>Solution to the Pop Quiz</h1>
<p>Now that we’ve refreshed our memories about classical measurement error, let’s a take a look at my pop quiz question from above:</p>
<blockquote>
<p>If <span class="math inline">\(D^*\)</span> and <span class="math inline">\(D\)</span> are binary random variables and <span class="math inline">\(D\)</span> is a noisy measure of <span class="math inline">\(D^*\)</span>, is it possible for the measurement error <span class="math inline">\(W \equiv D - D^*\)</span> to be <em>classical</em>? Explain why or why not.</p>
</blockquote>
<p>If <span class="math inline">\(W\)</span> is a classical measurement error then, among other things, it must be uncorrelated with <span class="math inline">\(D^*\)</span>.
But this is <em>impossible</em> if both <span class="math inline">\(D^*\)</span> and <span class="math inline">\(D\)</span> are binary.
By the definition of <span class="math inline">\(W\)</span>, <span class="math inline">\(D = D^* + W\)</span>.
If <span class="math inline">\(D^* = 1\)</span> then <span class="math inline">\(D = 1 + W\)</span>.
To ensure that <span class="math inline">\(D\)</span> takes on a value in <span class="math inline">\(\{0, 1\}\)</span>, this means that <span class="math inline">\(W\)</span> must be either <span class="math inline">\(0\)</span> or <span class="math inline">\(-1\)</span>.
If instead <span class="math inline">\(D^* = 0\)</span>, then <span class="math inline">\(D = W\)</span>, so <span class="math inline">\(W\)</span> must be either <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span>.
Hence, unless <span class="math inline">\(W\)</span> always equals zero, in which case there’s no measurement error, <span class="math inline">\(W\)</span> must always be <em>negatively correlated</em> with <span class="math inline">\(D^*\)</span>.
In other words, measurement error in a a binary variable can never be classical.
The same basic logic applies whenever <span class="math inline">\(X\)</span> and <span class="math inline">\(X^*\)</span> are bounded: to ensure that <span class="math inline">\(X\)</span> stays within its bounds, any measurement error must be correlated with <span class="math inline">\(X^*\)</span>.</p>
</div>
<div id="non-differential-measurement-error" class="section level1">
<h1>Non-Differential Measurement Error</h1>
<p>Classical measurement error, as we’ve seen, is a very special case.
Or to put it another way, non-classical measurement error isn’t as exotic as it sounds.
Because discrete random variables cannot be subject to classical measurement error, non-classical measurement error should be on any applied economist’s radar.
My next few posts will provide an overview of the simplest case: <em>non-differential</em> measurement error in a binary variable.
This assumption allows <span class="math inline">\(D^*\)</span> to be correlated with <span class="math inline">\(W\)</span>, but assumes that conditioning on <span class="math inline">\(D^*\)</span> is sufficient to <em>break</em> the dependence between <span class="math inline">\(W\)</span> and everything else.
Even in this relatively simple case, everything we’ve learned about classical measurement error goes out the window:</p>
<ol style="list-style-type: decimal">
<li>Non-differential measurement error does <em>not</em> necessarily cause attenuation.</li>
<li>The IV estimator doesn’t correct for non-differential measurement error, and a single instrument cannot serve “double-duty.”</li>
<li>Non-classical measurement error in the outcome variable generally <em>does</em> introduce bias.</li>
</ol>
<p>The good news is that there are methods to address non-differential measurement error.
In my next post, I’ll start by considering the case of a mis-measured binary outcome.</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Strictly speaking I haven’t used the assumption that <span class="math inline">\(W_X\)</span> is uncorrelated with <span class="math inline">\(X^*\)</span> in this derivation, but it’s implicit in the assumption that <span class="math inline">\(Z\)</span> is correlated with <span class="math inline">\(X^*\)</span> but not with <span class="math inline">\(W_X\)</span>.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
